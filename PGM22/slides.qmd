---
title: "You Only Derive Once (YODO):"
subtitle: "**Automatic Differentiation for Efficient Sensitivity Analysis in Bayesian Networks**"
author: "[Rafael Ballester-Ripoll](https://scholar.google.es/citations?user=zSdMQ0gAAAAJ) & [Manuele Leonelli](https://manueleleonelli.github.io/)"
date: 7th October 2022
institute: "IE University, Madrid (Spain)"
format:
    revealjs:
        theme: dark
type: website
output-dir: docs/
# incremental: true
width: 1700
height: 950
# include-in-header:
    # - preamble.tex
bibliography: references.bib
# csl: ieee.csl
---

## Motivation

- Bayesian networks are often learned from data
- Learned parameters $\pmb{\theta}$ in the conditional probability tables (CPT) may be uncertain

|                 	| Child = low 	| Child = medium 	| Child = high 	|
|-----------------	|-------------	|----------------	|--------------	|
| Parent = low    	| 0.3?         	| 0.4?            	| 0.3?          	|
| Parent = medium 	| 0.2?         	| 0.6?            	| 0.2?          	|
| Parent = high   	| 0.1?         	| 0.9?            	| 0?            	|

- Imagine a quantity of interest $g = P(Y_O = y_O \; | \; \pmb{Y}_E = \pmb{y}_E)$ arising from the BN
    <!-- - What is the probability of $Y_O = y_O$ given $\pmb{Y}_E = \pmb{y}_E$? -->
    - $Y_O$ is an output variable of interest; $\pmb{Y}_E$ are optional conditional evidential variables
    - E.g. $g =$  *"What is the risk of humanitarian disaster, if there's a large flood?"*
- Sensitivity analysis question: how does $g(\pmb{\theta})$ behave as $\pmb{\theta}$ changes?

## Proportional Covariation

<!-- - Consider two entries $\theta_i$ and $\theta_j$ of a CPT for the same configuration of the parents -->
- Let $\theta_i, \theta_{j_1}, \dots, \theta_{j_n}$ be the probabilities along a CPT row (must sum to 1)
- Problem: when we vary $\theta_i$ from its initial value $\theta_i^0$, the row will not sum to 1!
<!-- - We want to vary $\theta_i$  -->
- To fix that, proportional covariation [@laskey1995sensitivity] adjusts each $\theta_{j_k}$ as
$$\theta_{j_k}(\theta_i) = \frac{1-\theta_i}{1-\theta^0_i}\theta^0_{j_k}, \; k = 1, \dots, n$$
- If $\theta_i$ grows, everything else in the row shrinks proportionally
- PC is theoretically a natural choice [@leonelli2018geometric]
- For each $\theta_i$, PC yields an $f$:
$$f(\theta_i) = g(\theta_i, \theta_{j_1}(\theta_i), \dots, \theta_{j_n}(\theta_i))$$
<!-- - We assume it from now on in this presentation -->

## Sensitivity Values

<!-- - Let
\begin{cases}
f_{O}(\pmb{\theta}) := P(Y_O=y_O; \pmb{\theta}) \\
f_{O \cap E}(\pmb{\theta}) := P(Y_O=y_O \cap \pmb{Y}_E=\pmb{y}_E; \pmb{\theta}) \\
f_{O|E}(\pmb{\theta}) := f_O(\pmb{\theta}) / f_{O \cap E}(\pmb{\theta})
\end{cases} -->
- Let $f_{O|E}(\theta_i) := P(Y_O=y_O\;|\; \pmb{Y}_E=\pmb{y}_E; \theta_i) = \frac{P(Y_O=y_O; \theta_i)}{P(Y_O=y_O \cap \pmb{Y}_E=\pmb{y}_E; \theta_i)} = \frac{f_O(\theta_i)}{f_{O \cap E}(\theta_i)}$
- The *sensitivity value* [@van2007sensitivity] of a parameter $\theta_i$ is $|f'_{O|E}(\theta_i)|$
- The larger this value, the more our QoI is sensitive to changes in $\theta_i$
- It is a type of *local* sensitivity analysis
- Can be computed with e.g. finite differences
    - For example Euler: $$\frac{|f_{O|E}(\theta_i + h) - f_{O|E}(\theta_i)|}{h}$$
    - Requires two inference operations

## Computing Hyperbola Parameters $c_1, c_2, c_3, c_4$

- For unconditional probabilities, $f$ is just a straight line: $f_O(\theta_i)=c_1\theta_i+c_2$ where
$$
\begin{cases}
    c_1 = f_O'(\theta_i^0) \\
    c_2 = f_O(\theta_i^0) - c_1 \theta_i^0.
\end{cases}
$$

- For conditional probabilities, $f$ is the quotient of two lines (hyperbola) [@coupe2002properties]:
$$f_{O|E}(\theta_i) = \frac{f_O(\theta_i)}{f_{O \cap E}(\theta_i)} = \frac{c_1\theta_i+c_2}{c_3\theta_i+c_4}$$
amd then we can get the sensitivity value as
$$|f_{O|E}^{'}(\theta_i^0)| = \frac{|c_1 c_4 - c_2 c_3|}{(c_3\theta_i^0+c_4)^2}$$

## Vertex Proximity

:::: {.columns}

::: {.column width="70%"}

- Define
$$
\begin{cases}
s := -c_4 / c_3 \\
t := c_1 / c_3 \\
r := c_2 / c_3 + st
\end{cases}
$$
- Then, $f_{O|E}(\theta_i) = \frac{r}{\theta_i-s} + t$

- The *vertex proximity* [@van2007sensitivity] is $|\theta_i^0 - \theta_i^v|$, where
$$
\begin{equation}
	\theta_i^v = \left\{
	\begin{array}{ll}
	s+\sqrt{|r|}, & \mbox{if } s <0, \\
	s - \sqrt{|r|}, & \mbox{if } s > 0.
	\end{array}
	\right.
\end{equation}
$$

:::

::: {.column width="30%"}

```{python}
import matplotlib.pyplot as plt
import numpy as np
x = np.linspace(-1, 1.1, 100)
c1 = 0.5/20
c2 = 2/2-3
c3 = 1.5/2
c4 = 2.5/2-3
num = x*c1 + c2
den = x*c3 + c4
plt.plot(x, num/den-1)

s = -c4 / c3
t = c1 / c3
r = c2 / c3 + s*t
if s < 0:
    thetav = s + np.sqrt(np.abs(r))
else:
    thetav = s - np.sqrt(np.abs(r))
f = r / (thetav - s) + t - 1
plt.plot([thetav], [f], 'o')
plt.text(thetav+0.05, f, 'Vertex')
plt.text(thetav+0.02, 0.02, '$\\theta_i^v$')
plt.text(0+0.02, f+0.02, '$f(\\theta_i^v)$')
plt.plot([thetav, thetav], [0, f], '--', c='white')
plt.plot([0, thetav], [f, f], '--', c='white')
plt.xlabel('$\\theta_i$')
plt.xlim(0, 1)
plt.ylim(0, 1)
plt.gca().set_aspect('equal')
plt.style.use('dark_background')
```
:::
::::
- Being close to the vertex $\Rightarrow$ large changes in $\theta_i$ may have huge impact (even if $|f'(\theta_i)|$ is small)

# Problem: Slow For Large Networks

## Proposed Method: YODO

1. Compute $\nabla g_O$
2. For every $\theta_i$, derive $f'_O(\theta_i)$ and obtain $c_1, c_2$
3. Do analogously with $g_{O \cap E}(\pmb{\theta}) := P(Y_O = y_O \cap \pmb{Y}_E=\pmb{y}_E; \pmb{\theta})$ to obtain $c_3, c_4$
4. Compute sensitivity metrics using $c_1, \dots, c_4$

## Computing $\nabla g$ (I)

- We moralize the BN into a Markov Random Field

![](moralization.png){fig-align="center"}

## Computing $\nabla g$ (II)

- We encode the evidence $Y_O = y_O$ (and $\pmb{Y}_E = \pmb{y}_E$ if needed) in the MRF potentials
- How? We set to 0 the appropriate entries
- Example (parent = $Y_1$, child = $Y_2$, we set $Y_2 = 3$):

![](set_evidence.png){fig-align="center"}

## Computing $\nabla g$ (III)

- We do inference via variable elimination: eliminate all nodes to get a scalar $x$

![](marginalization.png){fig-align="center"}

(*forward pass* in machine learning)

## Computing $\nabla g$ (IV)

- We backpropagate to obtain the gradient w.r.t. of $g$ all $\pmb{\theta}$
    - *Backward pass* in machine learning
- Computed via chain rule to machine precision (not numerical or symbolic differentiation)
- Cost: same complexity as the inference step [@liaoDifferentiableProgramming2019]
- Software-wise very easy thanks to automatic differentiation
    1. Compute your QoI `x` normally
    2. Call a function like `x.backward()`
    3. The gradient entries are created side-by-side the original tensors (CPTs in our case)

## Last: Finding $f'(\theta_i)$ from $\nabla g$

- Recall that $f$ is
$$f(\theta_i) = g(\theta_i, \theta_{j_1}(\theta_i), \dots, \theta_{j_n}(\theta_i))$$
<!-- where $\theta_{j_k}(\theta_i) = \frac{1-\theta_i}{1-\theta^0_i}\theta^0_j$ -->
- Then (generalized chain rule) we have
$$	f'(\theta_i) = \frac{\partial g}{\partial \theta_i} - \frac{(\partial g / \partial \theta_{j_1}) \cdot \theta_{j_1}^0 + \dots + (\partial g / \partial \theta_{j_n}) \cdot \theta_{j_n}^0}{1 - \theta_i^0}$$
- Note: the partials of $g$ do not depend on the specific CPT entry $\theta_i$ we consider
- We only need to compute $\nabla g$ once

<!-- :::{.callout-note}
The partials of $g$ do not depend on the specific CPT entry $\theta_i$ we're looking at $\rightarrow$ only need to compute them once
::: -->

# Experiments

- One [learned network](https://github.com/rballester/yodo/blob/main/networks/bn_pgm_free2.bif) (21 nodes)
- 10 Bayesian networks [from bnlearn](https://www.bnlearn.com/bnrepository/) (up to 441 nodes)

## Humanitarian Crises Network

![](Rplot.png){fig-align="center"}

## Results (Humanitarian Crises)

- Top 15 parameters for $f = P(\mbox{RISK} = \mbox{high} \;|\; \mbox{EARTHQUAKE} = \mbox{high})$:
```{python}
#| echo: true
#| out.width=50%
import yodo
import pgmpy.readwrite
g = pgmpy.readwrite.BIFReader("bn_pgm_free2.bif").get_model()
yodo.plot(g, {'RISK': 'high'}, given={'EARTHQUAKE': 'high'}, figsize=(10, 5), nbars=15)
```
- The full computation (183 parameters) took 0.055s

## Results (bnlearn Models)

![](results_table.png){fig-align="center"}

- Speed-up vs. finite differences: 2 - 4 orders of magnitude

## Open-source Implementation

- Available in Python: [github.com/rballester/yodo](https://github.com/rballester/yodo)
- Dependencies:
<!-- - [[NumPy](https://www.numpy.org)]{.smallcaps} -->
    - [[pgmpy](https://pgmpy.org/)]{.smallcaps} for reading and moralizing Bayesian networks
    - [[opt_einsum](https://optimized-einsum.readthedocs.io/en/stable/)]{.smallcaps} for efficient tensor contraction
        - Equivalent to variable elimination [@RS:18]
    - [[PyTorch](https://pytorch.org/)]{.smallcaps} as autodifferentiation backend for manipulating tensors
    - [[gmtorch](https://github.com/rballester/gmtorch)]{.smallcaps} for operations with Markov random fields

- Upcoming: [github.com/manueleleonelli/bnmonitor](https://github.com/manueleleonelli/bnmonitor)

# Takeaways

<!-- - Thanks to modern tools, evaluation is automatic and does not depend on the graph topology -->
- Autodifferentiation is very promising for local sensitivity analysis
- Ability to process huge graphs in seconds
- Extensible to other inference algorithms, as long as they are differentiable
- Bridges between tensor networks and PGMs are worth exploring

## Thank You For Listening

<!-- :::: {.columns}

::: {.column width="40%"}
[rafael.ballester@ie.edu](rafael.ballester@ie.edu)
:::

::: {.column width="40%"}
[manuele.leonelli@ie.edu](manuele.leonelli@ie.edu)
:::

::::

<p class="text-center">
[rafael.ballester@ie.edu](rafael.ballester@ie.edu)
[manuele.leonelli@ie.edu](manuele.leonelli@ie.edu)
</p> -->

<!-- ### References -->

::: {#refs}
:::
<br>

### IE University is Hiring!

- Tenure-track positions in Computer Science and Information Technology
- Main campus in Madrid, Spain

![](qr.png){fig-align="left" width="55%"}

[https://apply.interfolio.com/107871](https://apply.interfolio.com/107871)

<!-- ## Columns Test

:::: {.columns}

::: {.column width="40%"}
Left column

- Doing things
- Doing things
- Doing things
- Doing things
- Doing things
- Doing things
:::

:::: -->