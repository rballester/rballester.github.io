---
title: "You Only Derive Once (YODO):"
subtitle: "**Automatic Differentiation for Efficient Sensitivity Analysis in Bayesian Networks**"
author: "[Rafael Ballester-Ripoll](https://scholar.google.es/citations?user=zSdMQ0gAAAAJ) & [Manuele Leonelli](https://manueleleonelli.github.io/)"
date: 7th October 2022
institute: "IE University, Madrid (Spain)"
format:
    revealjs:
        theme: dark
# incremental: true FIXME
width: 1700
height: 900
# include-in-header:
    # - preamble.tex
bibliography: references.bib
# csl: ieee.csl
---

## Motivation

- Bayesian networks are often learned from data
- Learned parameters $\pmb{\theta}$ in the conditional probability tables (CPT) may be uncertain

|                 	| Child = low 	| Child = medium 	| Child = high 	|
|-----------------	|-------------	|----------------	|--------------	|
| Parent = low    	| 0.3?         	| 0.4?            	| 0.3?          	|
| Parent = medium 	| 0.2?         	| 0.6?            	| 0.2?          	|
| Parent = high   	| 0.1?         	| 0.9?            	| 0?            	|

- Imagine a quantity of interest $f$ arising from the BN
    - What is the probability of $Y_O = y_O$ given $\pmb{Y}_E = \pmb{y}_E$?
    - E.g. $f =$  *"What is the risk of humanitarian disaster, if there's a large flood?"*
- Sensitivity analysis question: how does $f(\pmb{\theta})$ behave as $\pmb{\theta}$ changes?

## Sensitivity Values

<!-- - Let
\begin{cases}
f_{O}(\pmb{\theta}) := P(Y_O=y_O; \pmb{\theta}) \\
f_{O \cap E}(\pmb{\theta}) := P(Y_O=y_O \cap \pmb{Y}_E=\pmb{y}_E; \pmb{\theta}) \\
f_{O|E}(\pmb{\theta}) := f_O(\pmb{\theta}) / f_{O \cap E}(\pmb{\theta})
\end{cases} -->
- Let $f_{O|E}(\pmb{\theta}) := P(Y_O=y_O\;|\; \pmb{Y}_E=\pmb{y}_E; \pmb{\theta}) = \frac{P(Y_O=y_O; \pmb{\theta})}{P(Y_O=y_O \cap \pmb{Y}_E=\pmb{y}_E; \pmb{\theta})} = \frac{f_O(\pmb{\theta})}{f_{O \cap E}(\pmb{\theta})}$
- The *sensitivity value* [@van2007sensitivity] of a parameter $\theta_i$ is $|f'_{O|E}(\theta_i)|$
- The larger this value, the more our QoI is sensitive to changes in $\theta_i$
- It is a type of *local* sensitivity analysis
- Can be computed with e.g. finite differences

:::{.callout-warning appearance="simple"}
Problem 1: when we vary $\theta_i$, the row will not sum to 1
:::

:::{.callout-warning appearance="simple"}
Problem 2: can we find most influential parameters? If there are many, this may be slow
:::

## Fixing Problem 1: Proportional Covariation

- Consider two entries $\theta_i$ and $\theta_j$ of a CPT for same configuration of the parents
- We want to vary $\theta_i$ from its initial value $\theta_i^0$
- To preserve the sum-to-1 property, proportional covariation [@laskey1995sensitivity] adjusts $\theta_j$ as
$$\theta_j(\theta_i) = \frac{1-\theta_i}{1-\theta^0_i}\theta^0_j$$
- If $\theta_i$ grows, everything else in the row shrinks proportionally
- PC is theoretically a natural choice [@chan2005distance] [@leonelli2018geometric]
<!-- - We assume it from now on in this presentation -->

## Computing Hyperbola Parameters $c_1, c_2, c_3, c_4$

- For unconditional probabilities, $f$ is just a straight line: $f_O(\theta_i)=c_1\theta_i+c_2$ where
$$
\begin{cases}
    c_1 = f_O'(\theta_i^0) \\
    c_2 = f_O(\theta_i^0) - c_1 \theta_i^0.
\end{cases}
$$

- For conditional probabilities, $f$ is the quotient of two lines (hyperbola) [@coupe2002properties]:
$$	f_{O|E}(\theta_i)=\frac{c_1\theta_i+c_2}{c_3\theta_i+c_4} = \frac{f_O(\theta_i)}{f_{O \cap E}(\theta_i)}$$
amd then we can get the sensitivity value as
$$|f_{O|E}^{'}(\theta_i^0)| = \frac{|c_1 c_4 - c_2 c_3|}{(c_3\theta_i^0+c_4)^2}$$

## Vertex Proximity

:::: {.columns}

::: {.column width="50%"}



- Define
$$
\begin{cases}
s := -c_4 / c_3 \\
t := c_1 / c_3 \\
r := c_2 / c_3 + st
\end{cases}
$$
- Then, $f_{O, E}(\theta_i) = \frac{r}{\theta_i-s} + t$

- The *vertex proximity* [@van2007sensitivity] is $|\theta_i^0 - \theta_i^v|$, where
$$
\begin{equation}
	\theta_i^v = \left\{
	\begin{array}{ll}
	s+\sqrt{|r|}, & \mbox{if } s <0, \\
	s - \sqrt{|r|}, & \mbox{if } s > 0.
	\end{array}
	\right.
\end{equation}
$$

:::

::: {.column width="50%"}

```{python}
import matplotlib.pyplot as plt
import numpy as np
x = np.linspace(-1, 1.1, 100)
c1 = 0.5/20
c2 = 2/2-3
c3 = 1.5/2
c4 = 2.5/2-3
num = x*c1 + c2
den = x*c3 + c4
plt.plot(x, num/den)

s = -c4 / c3
t = c1 / c3
r = c2 / c3 + s*t
if s < 0:
    thetav = s + np.sqrt(np.abs(r))
else:
    thetav = s - np.sqrt(np.abs(r))
f = r / (thetav - s) + t
plt.plot([thetav], [f], 'o')
plt.text(thetav+0.05, f, 'Vertex')
plt.text(thetav+0.02, 1.02, '$\\theta_i^v$')
plt.text(0+0.02, f+0.02, '$f(\\theta_i^v)$')
plt.plot([thetav, thetav], [0, f], '--', c='black')
plt.plot([0, thetav], [f, f], '--', c='black')
plt.xlim(0, 1)
plt.ylim(1, 2)
plt.gca().set_aspect('equal')
```
:::
::::
- Even if $|f'|$ is small, being close to the vertex means larger changes in $\theta_i$ could have huge impact

# Fixing Problem 2: The YODO Method

## Algorithm Summary

1. Define $g_O(\pmb{\theta}) := P(Y_O = y_O ; \pmb{\theta})$ (like $f_O$, just without proportional covariation)
2. Compute $\nabla g_O$
3. For every $\theta_i$, derive $f'_O(\theta_i)$ and extract $c_1, c_2$
4. Do analogously with $g_{O \cap E}(\pmb{\theta}) := P(Y_O = y_O \cap \pmb{Y}_E=\pmb{y}_E; \pmb{\theta})$ and extract $c_3, c_4$
5. Compute sensitivity metrics using $c_1, \dots, c_4$

## Computing $\nabla g$ (I)

- We moralize the BN into a Markov Random Field

![](moralization.png){fig-align="center"}

## Computing $\nabla g$ (II)

- We encode the evidence $Y_O = y_O$ in the MRF potentials
- Example (parent = $Y_1$, child = $Y_2$, we set $Y_2 := 3$):

![](set_evidence.png){fig-align="center"}

## Computing $\nabla g$ (III)

- We do inference via variable elimination: eliminate all nodes to get a scalar

![](marginalization.png){fig-align="center"}

(*forward pass* in machine learning)

## Computing $\nabla g$ (IV)

- We backpropagate to obtain the gradient w.r.t. all $\pmb{\theta}$
    - *Backward pass* in machine learning
- Computed via rule of chain (not numerical or symbolic differentiation)
- Cost: same complexity as the inference step [@liaoDifferentiableProgramming2019]
- Software-wise very easy: compute your QoI `x`, then run `x.backward()`

## Finding $f'(\theta_i)$ out of $\nabla g$

- Let $\theta_i, \theta_{j_1}, \dots, \theta_{j_n}$ be the probabilities along a CPT row (must sum to 1)
- We rewrite $f$ as
$$f(\theta_i) = g(\theta_i, \theta_{j_1}(\theta_i), \dots, \theta_{j_n}(\theta_i))$$
<!-- where $\theta_{j_k}(\theta_i) = \frac{1-\theta_i}{1-\theta^0_i}\theta^0_j$ -->
- Then (generalized chain rule) we have
$$	f'(\theta_i) = \frac{\partial g}{\partial \theta_i} - \frac{(\partial g / \partial \theta_{j_1}) \cdot \theta_{j_1}^0 + \dots + (\partial g / \partial \theta_{j_n}) \cdot \theta_{j_n}^0}{1 - \theta_i^0}$$
- Note: the partials of $g$ do not depend on the specific CPT entry $\theta_i$ we consider
- We only need to compute $\nabla g$ once

<!-- :::{.callout-note}
The partials of $g$ do not depend on the specific CPT entry $\theta_i$ we're looking at $\rightarrow$ only need to compute them once
::: -->

## In Code...

```{python}
#| execute: false
#| echo: true
# p = graph[[]]
# print(p)
# p.backward()
```

# Experiments

- One [learned network](https://github.com/rballester/yodo/blob/main/networks/bn_pgm_free2.bif) (21 nodes)
- 10 Bayesian networks [from bnlearn](https://www.bnlearn.com/bnrepository/) (up to 441 nodes)

## Software

Python implementation with libraries:

<!-- - [[NumPy](https://www.numpy.org)]{.smallcaps} -->
- [[pgmpy](https://pgmpy.org/)]{.smallcaps} for reading and moralizing Bayesian networks
- [[PyTorch](https://pytorch.org/)]{.smallcaps} as numerical and autodifferentiation backend
- [[opt_einsum](https://optimized-einsum.readthedocs.io/en/stable/)]{.smallcaps} for efficient marginalization
- [[gmtorch](https://github.com/rballester/gmtorch)]{.smallcaps} for operations with Markov random fields

Open-sourced in [github.com/rballester/yodo](https://github.com/rballester/yodo)

- Upcoming: [github.com/manueleleonelli/bnmonitor](https://github.com/manueleleonelli/bnmonitor)

## Humanitarian Crises Network

![](Rplot.png){fig-align="center"}

```{python}
# import pgmpy.readwrite
# import yodo
# import matplotlib.pyplot as plt

# g = pgmpy.readwrite.BIFReader("networks/hailfinder.bif").get_model()
```

## Results (Humanitarian Crises)

Top 20 parameters for $f = P(\mbox{RISK} = \mbox{high} \;|\; \mbox{EARTHQUAKE} = \mbox{high})$:

![](humanitarian_results.png){fig-align="center"}

- Whole computation (183 parameters) took 0.055s

## Results (bnlearn Models)

![](results_table.png){fig-align="center"}

- Speed-up of 2 to 4 orders of magnitude

## Conclusions

<!-- - Thanks to modern tools, evaluation is automatic and does not depend on the graph topology -->
- Autodifferentiation is very promising for local sensitivity analysis
- Ability to process huge graphs in seconds
- Extensible to other inference algorithms, as long as they are differentiable
- It is worth exploring the bridges between tensor networks and PGMs

## Thanks For Listening!

:::: {.columns}

::: {.column width="40%"}
[rafael.ballester@ie.edu](rafael.ballester@ie.edu)
:::

::: {.column width="40%"}
[manuele.leonelli@ie.edu](manuele.leonelli@ie.edu)
:::

::::

<p class="text-center">
[rafael.ballester@ie.edu](rafael.ballester@ie.edu)
[manuele.leonelli@ie.edu](manuele.leonelli@ie.edu)
</p>

### References

<!-- ## Columns Test

:::: {.columns}

::: {.column width="40%"}
Left column

- Doing things
- Doing things
- Doing things
- Doing things
- Doing things
- Doing things
:::

:::: -->